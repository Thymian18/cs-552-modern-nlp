{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3275ace2",
   "metadata": {},
   "source": [
    "# ðŸ“š  Exercise Session - Week 2\n",
    "\n",
    "Welcome to Week 2 exercise's session of CS552-Modern NLP!\n",
    "\n",
    "\n",
    "> **What will be covered:**\n",
    "1. [**TASK A:** N-gram Language Models](#ngram_lm)\n",
    "    - [Unigram Language Model](#unigram_lm)\n",
    "    - [Bi-gram Language Model](#bigram_lm)\n",
    "    - [Tri-gram Language Model](#trigram_lm)\n",
    "     \n",
    "2. [**TASK B:** Neural Language Models](#neural_lm)\n",
    "    - [Fixed-Window Neural Language Model](#fixed_window_lm)\n",
    "    - [RNN-based Language Model](#rnn_lm)\n",
    "\n",
    "> **By the end of the session you will be able to:**\n",
    "> - âœ…  Compute and interpret the perplexity of a language model \n",
    "> - âœ…  Implement N-gram language models for N=1,2,3\n",
    "> - âœ…  Implement, train, and evaluate a fixed window language model\n",
    "> - âœ…  Evaluate an RNN language model\n",
    "> - âœ…  Understand the advantages and disadvantages of each of the above models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98eb6b6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T07:57:33.846073Z",
     "start_time": "2024-03-01T07:57:33.842570Z"
    }
   },
   "outputs": [],
   "source": [
    "# install the libraries if needed.\n",
    "# !pip install datasets\n",
    "# !pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272c0a7c",
   "metadata": {},
   "source": [
    "<a name=\"ngram_lm\"></a>\n",
    "## 1. Task A: N-gram Language Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c137c6",
   "metadata": {},
   "source": [
    "\n",
    "In this exercise, we will better understand the functioning of different types of (non-neural) language modeling, namely,  Unigram LM, Bi-gram LM, and Tri-gram LM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066c0427",
   "metadata": {},
   "source": [
    "### 1.1 Unigram Language Model <a name=\"unigram_lm\"></a>\n",
    "In the simple Unigram language model, we pick/generate next token independent of the previous token. In other words, during the generation, we pick the tokens according to the token probability. Therefore, for an arbitrary sequence $x_1x_2~...x_n$, its respective probability becomes:\n",
    "$$p(x_1x_2~...x_n) = \\Pi_{i=1} ^n p(x_i)$$\n",
    "Let's use an unsupervised dataset (raw corpus) to evaluate this model's perplexity. We use Huggingface's `datasets` library to download needed datasets.\n",
    " \n",
    "\n",
    "Here we use the `Penn Treebank` dataset, featuring a million words of 1989 Wall Street Journal material. The rare words in this version are already replaced with `<unk>` token. The numbers are also replaced with a special token. This token replacement helps us to end up with a more reasonable vocabulary size to work with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69afb9de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T07:57:39.181513Z",
     "start_time": "2024-03-01T07:57:33.848093Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ptb_text_only (C:/Users/Damian Kopp/.cache/huggingface/datasets/ptb_text_only/penn_treebank/1.1.0/8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f)\n",
      "Loading cached split indices for dataset at C:\\Users\\Damian Kopp\\.cache\\huggingface\\datasets\\ptb_text_only\\penn_treebank\\1.1.0\\8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f\\cache-6d72a72d5c72be3d.arrow and C:\\Users\\Damian Kopp\\.cache\\huggingface\\datasets\\ptb_text_only\\penn_treebank\\1.1.0\\8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f\\cache-3e2d0e2b1466183a.arrow\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import datasets\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "ptb_dataset = load_dataset(\"ptb_text_only\", split=\"train\")\n",
    "\n",
    "# splitting dataset in train/test (to be later used for language model evaluation)\n",
    "ptb_dataset = ptb_dataset.train_test_split(test_size=0.2, seed=1)\n",
    "ptb_train, ptb_test = ptb_dataset['train'], ptb_dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20589cd0",
   "metadata": {},
   "source": [
    "#### Let's have a look at a few samples of the training dataset (and also the structure of the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6ceb684",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T07:57:39.189213Z",
     "start_time": "2024-03-01T07:57:39.183701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': \"a former executive agreed that the departures do n't reflect major problems adding if you see any company that grows as fast as reebok did it is going to have people coming and going\"}\n",
      "\n",
      "{'sentence': 'with talk today of a second economic <unk> in west germany east germany no longer can content itself with being the economic star in a loser league'}\n",
      "\n",
      "{'sentence': 'transportation secretary sam skinner who earlier fueled the anti-takeover fires with his <unk> attacks on foreign investment in u.s. carriers now says the bill would further <unk> the jittery capital markets'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"{ptb_train[0]}\\n\\n{ptb_train[1]}\\n\\n{ptb_train[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07207b13",
   "metadata": {},
   "source": [
    "During generation with a given language model, we often need to have a `<stop>` token in our vocabulary to terminate the generation of a given sentence/paragraph. In this dataset, every sample is a sentence, and the `<stop>` token should be added to the end of every sample (i.e., end of sentence).\n",
    "\n",
    "#### Create a new train/test dataset starting from `ptb_train` and `ptb_test` that has a `<stop>` at the end of each sentence. (Note: do not change the structure of the datasets objects, and just change the respective sentences as discussed).\n",
    "Hint: use the `.map()` functionality of the `datasets` package (read more [here](https://huggingface.co/docs/datasets/process#map]))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa1b7b70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T07:57:39.254163Z",
     "start_time": "2024-03-01T07:57:39.190742Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Damian Kopp\\.cache\\huggingface\\datasets\\ptb_text_only\\penn_treebank\\1.1.0\\8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f\\cache-023b2f25779c4af5.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Damian Kopp\\.cache\\huggingface\\datasets\\ptb_text_only\\penn_treebank\\1.1.0\\8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f\\cache-8c9fb89d1a72f591.arrow\n"
     ]
    }
   ],
   "source": [
    "def add_stop_token(input_sample: dict):\n",
    "    '''\n",
    "    args:\n",
    "        input_sample: a dict representing a sample of the dataset. (look above for the dict struture)\n",
    "    output:\n",
    "        modified_sample: modified dict adding <stop> at the end of each sentence.\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    modified_sample = {'sentence': input_sample['sentence'] + \" <stop>\"}\n",
    "    \n",
    "    return modified_sample\n",
    "    \n",
    "    \n",
    "ptb_train = ptb_train.map(add_stop_token)\n",
    "ptb_test = ptb_test.map(add_stop_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d338821",
   "metadata": {},
   "source": [
    "For both `ptb_train` and `ptb_test` datasets, filter out every sample that has less than 3 tokens. it will help remove very short sentences that are not very helpful for training/evaluating a langugage model.\n",
    "\n",
    "Hint: use `.filter()` functionality of the `datasets` package (read more [here](https://huggingface.co/docs/datasets/process#select-and-filter))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c0222f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T07:57:39.266107Z",
     "start_time": "2024-03-01T07:57:39.256169Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Damian Kopp\\.cache\\huggingface\\datasets\\ptb_text_only\\penn_treebank\\1.1.0\\8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f\\cache-f22280d72859a42b.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Damian Kopp\\.cache\\huggingface\\datasets\\ptb_text_only\\penn_treebank\\1.1.0\\8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f\\cache-c29dbf1acdd974b3.arrow\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "ptb_train = ptb_train.filter(lambda sample: len(sample['sentence'].split()) >= 3)\n",
    "ptb_test = ptb_test.filter(lambda sample: len(sample['sentence'].split()) >= 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2733f77b",
   "metadata": {},
   "source": [
    "#### What are the 10 most frequent tokens in this dataset? Can you spot the token used to replace the numbers in this dataset? How are rare tokens replaced in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28e93fb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T07:59:13.070389Z",
     "start_time": "2024-03-01T07:59:11.632029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 40616), ('<unk>', 35888), ('<stop>', 33539), ('N', 25966), ('of', 19459), ('to', 18896), ('a', 16901), ('in', 14473), ('and', 14013), (\"'s\", 7850)]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "from collections import Counter\n",
    "\n",
    "# 10 most frequent tokens\n",
    "word_counts = Counter()\n",
    "for sample in ptb_train:\n",
    "    word_counts.update(sample['sentence'].split())\n",
    "print(word_counts.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eaec09",
   "metadata": {},
   "source": [
    "#### Now let's create a dictionary of the word probabilites (in the format of `{word: Prob(word)}`in the following function. We will use these probabilities to estimate sequence probabilities for a given sequence, as mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e015a74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T08:00:04.014995Z",
     "start_time": "2024-03-01T08:00:02.629450Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "def get_word_probability_dict(train_dataset: datasets.arrow_dataset.Dataset):\n",
    "    '''\n",
    "    args: \n",
    "        train_dataset: a Dataset object that can be iterated to get all the sentences\n",
    "    output:\n",
    "        word_prob_dict: a dictionary containing the word probabilities (and outputing zero for non-seen tokens)\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    word_counts = Counter()\n",
    "    for sample in train_dataset:\n",
    "        word_counts.update(sample['sentence'].split())\n",
    "    dataset_size = word_counts.total()\n",
    "    \n",
    "    count_to_prob = lambda value: value / dataset_size\n",
    "    word_prob_dict = dict(word_counts)\n",
    "    word_prob_dict = dict(map(lambda kv: (kv[0], count_to_prob(kv[1])), word_prob_dict.items()))\n",
    "    \n",
    "    return word_prob_dict\n",
    "\n",
    "word_prob_dict = get_word_probability_dict(ptb_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766d403e",
   "metadata": {},
   "source": [
    "Let's also get a sense of how high the top-k probabilities are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37d958f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T08:00:04.386865Z",
     "start_time": "2024-03-01T08:00:04.373260Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0.054680915800329036),\n",
       " ('<unk>', 0.04831565654525823),\n",
       " ('<stop>', 0.045153221268151356),\n",
       " ('N', 0.03495776688180381),\n",
       " ('of', 0.026197457665910053),\n",
       " ('to', 0.025439496379826114),\n",
       " ('a', 0.022753647772832404),\n",
       " ('in', 0.019484855583468637),\n",
       " ('and', 0.01886556217032723),\n",
       " (\"'s\", 0.010568376724260954),\n",
       " ('for', 0.009569429523063295),\n",
       " ('that', 0.009558659202834748),\n",
       " ('$', 0.00801984970018121),\n",
       " ('is', 0.007975422129238458),\n",
       " ('it', 0.006556432439127497),\n",
       " ('said', 0.006516043738270448),\n",
       " ('on', 0.006050227388385825),\n",
       " ('at', 0.005312460452730411),\n",
       " ('by', 0.005303036422530433),\n",
       " ('as', 0.00519667951027354)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_prob_dict.items(), key=lambda item: item[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b408c2",
   "metadata": {},
   "source": [
    "#### Now let's analyze the Unigram language model for different sequences. We first create a function that can output the probability for a given string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc167228",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T08:00:11.154612Z",
     "start_time": "2024-03-01T08:00:11.149131Z"
    }
   },
   "outputs": [],
   "source": [
    "def unigram_lm_seq_probability(input_sentence: str,\n",
    "                               word_prob_dict: dict):\n",
    "    '''\n",
    "    args:\n",
    "        input_sentence: The input sequence string. Here we assume\n",
    "        word_prob_dict: A dictionary containing the probability for a given token\n",
    "    output:\n",
    "        probability: The probability of the input_sentence according to the Unigram language model\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    words = np.array(input_sentence.split())\n",
    "    look_up = np.vectorize(lambda word: word_prob_dict.get(word, 1))\n",
    "    probs = look_up(words)\n",
    "    \n",
    "    probability = np.prod(probs)\n",
    "    \n",
    "    return probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3427e046",
   "metadata": {},
   "source": [
    "#### Let's investigate a major issue with Unigram language model. What are the probabilities for the two following sequences?\n",
    "- the the the the \\<stop>\n",
    "- i love computer science \\<stop>\n",
    "\n",
    "Discussion: How can we avoid having large probability values for sequences like `the the the <stop>`\n",
    "> Maybe by somehow decreasing the probability (with a penalty term) of word repetitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c753d2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T08:00:14.184472Z",
     "start_time": "2024-03-01T08:00:14.178953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability for seq1 is 4.0367500274713214e-07, and for seq2 is 2.3485816949091947e-17\n"
     ]
    }
   ],
   "source": [
    "seq1 = \"the the the the <stop>\"\n",
    "seq2 = \"i love computer science <stop>\"\n",
    "\n",
    "prob_seq1 = unigram_lm_seq_probability(seq1, word_prob_dict)\n",
    "prob_seq2 = unigram_lm_seq_probability(seq2, word_prob_dict)\n",
    "print(f\"probability for seq1 is {prob_seq1}, and for seq2 is {prob_seq2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06747969",
   "metadata": {},
   "source": [
    "#### Now let's formally evaluate the Unigram model in terms of perplexity. We first compute the entropy as the average negative log-likelihood:\n",
    "$$H(W_{test}âˆ£M)= \\frac{1}{|W_{test}|} \\sum_{w\\in W_{test}} âˆ’log_2P(wâˆ£M)$$\n",
    ", where $W_{test}$ is the input sequence and M is the Unigram language model. (note that the logarithm is in base 2).\n",
    "\n",
    "In order to get a reliable value, we will do the above calculation for all the sentences in `ptb_test` dataset and then an average is taken over all these samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa48300f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T08:00:24.527850Z",
     "start_time": "2024-03-01T08:00:24.521276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy for seq1 is 4.248060482473582, and for seq2 is 11.048197566955974\n"
     ]
    }
   ],
   "source": [
    "def get_unigram_lm_entropy(input_sentence: str,\n",
    "                           word_prob_dict: dict):\n",
    "    '''\n",
    "    args:\n",
    "        input_sentence: the input string that we would like to have its respective entropy value.\n",
    "        word_prob_dict: A dictionary containing the probability for a given token\n",
    "    output:\n",
    "        entropy: entropy value as defined above\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    words = np.array(input_sentence.split())\n",
    "    summand_compute = np.vectorize(lambda word: -np.log2(word_prob_dict.get(word, 1)))\n",
    "    \n",
    "    entropy = np.mean(summand_compute(words))\n",
    "\n",
    "    return entropy\n",
    "\n",
    "prob_seq1 = get_unigram_lm_entropy(seq1, word_prob_dict)\n",
    "prob_seq2 = get_unigram_lm_entropy(seq2, word_prob_dict)\n",
    "print(f\"Entropy for seq1 is {prob_seq1}, and for seq2 is {prob_seq2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07afa63",
   "metadata": {},
   "source": [
    "Now compute the average entropy for all the sentences in the `ptb_test` given above function, and then compute the average entropy. Then compute the perplexity as $2^{\\bar{H}}$, where $\\bar{H}$ is the average perplexity over the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac66be2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T08:00:26.559244Z",
     "start_time": "2024-03-01T08:00:25.817156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity for the Unigram language model is 677.4720975633206\n"
     ]
    }
   ],
   "source": [
    "def get_unigram_lm_perplexity(test_dataset: datasets.arrow_dataset.Dataset,\n",
    "                              word_prob_dict: dict):\n",
    "    '''\n",
    "    args:\n",
    "        test_dataset: the test dataset samples are used to compute the perplexity for the Unigram LM.\n",
    "        word_prob_dict: A dictionary containing the probability for a given token\n",
    "    output:\n",
    "        perplexity: entropy value as defined above\n",
    "    '''  \n",
    "    # YOUR CODE HERE       \n",
    "    test_dataset = np.array(test_dataset)\n",
    "    compute_entropy = np.vectorize(lambda sample: get_unigram_lm_entropy(sample['sentence'], word_prob_dict))\n",
    "    sentence_entropies = compute_entropy(test_dataset)\n",
    "    \n",
    "    perplexity = np.power(2, np.mean(sentence_entropies))\n",
    "\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "unigram_lm_perplexity = get_unigram_lm_perplexity(ptb_test, word_prob_dict)\n",
    "print(f\"The perplexity for the Unigram language model is {unigram_lm_perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3cec0a",
   "metadata": {},
   "source": [
    "As discussed in the lectures, the models with lower perplexities are desired; however, we should be careful when comparing language models with different vocabualry sizes.\n",
    "#### In the `ptb_train` dataset, replace every token that is appearing less than 10 times with the `<unk>` token. (Note: the same token replacement should be done for the test dataset). What is the Unigram language model perplexity for the new dataset?\n",
    "Discussion: What would happen to the vocabulary size and perplexity as we increase the rare token threshold to higher values? (instead of 10 here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a14c853",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T08:00:28.932173Z",
     "start_time": "2024-03-01T08:00:26.963921Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Damian Kopp\\.cache\\huggingface\\datasets\\ptb_text_only\\penn_treebank\\1.1.0\\8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f\\cache-6750327a9745d6a3.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Damian Kopp\\.cache\\huggingface\\datasets\\ptb_text_only\\penn_treebank\\1.1.0\\8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f\\cache-aab7acb28cbdeb52.arrow\n"
     ]
    }
   ],
   "source": [
    "def remove_rare_token(train_dataset: datasets.arrow_dataset.Dataset,\n",
    "                      test_dataset: datasets.arrow_dataset.Dataset,\n",
    "                      rare_token_threshold: int):\n",
    "    '''\n",
    "    Note that the tokens that are considered rare here, are identified based on the train_dataset, so that\n",
    "    we have the same token mapping (to <unk>) for both the train and test datasets. \n",
    "    args:\n",
    "        train_dataset: the input dataset where its rare tokens has to be replaced with <unk> token.\n",
    "        rare_token_threshold: every word that is appearing less than this threshold in the train dataset will\n",
    "                              be replace with the <unk> token\n",
    "    output:\n",
    "        cleaned_train_dataset: the cleaned train dataset where rare tokens are replace with <unk> token.\n",
    "        cleaned_test_dataset: the cleaned test dataset where rare tokens are replace with <unk> token.\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    train_word_counts = Counter()\n",
    "    for sample in train_dataset:\n",
    "        train_word_counts.update(sample['sentence'].split())\n",
    "        \n",
    "    test_word_counts = Counter()\n",
    "    for sample in test_dataset:\n",
    "        test_word_counts.update(sample['sentence'].split())\n",
    "            \n",
    "    cleaned_train_dataset = train_dataset.map(lambda sample: {'sentence': ' '.join(['<unk>' if train_word_counts.get(word, 0) < 10 else word for word in sample['sentence'].split()])})\n",
    "    cleaned_test_dataset = test_dataset.map(lambda sample: {'sentence': ' '.join(['<unk>' if test_word_counts.get(word, 0) < 10 else word for word in sample['sentence'].split()])})\n",
    "    \n",
    "    return cleaned_train_dataset, cleaned_test_dataset\n",
    "\n",
    "cleaned_train_dataset, cleaned_test_dataset = remove_rare_token(train_dataset=ptb_train,\n",
    "                                                                test_dataset=ptb_test,\n",
    "                                                                rare_token_threshold=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f09a7d3",
   "metadata": {},
   "source": [
    "##### Now, follow similar steps to compute the perplexity given the two new datasets (`cleaned_train_dataset` and `cleaned_test_dataset`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2083f50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T08:00:30.209330Z",
     "start_time": "2024-03-01T08:00:28.935182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity for the Unigram language model after replacing rare tokens is  237.83971555221527\n"
     ]
    }
   ],
   "source": [
    "cleaned_unigram_lm_perplexity = -1\n",
    "\n",
    "# YOUR CODE HERE\n",
    "word_prob_dict = get_word_probability_dict(cleaned_train_dataset)\n",
    "cleaned_unigram_lm_perplexity = get_unigram_lm_perplexity(cleaned_test_dataset, word_prob_dict)\n",
    "\n",
    "print(\"The perplexity for the Unigram language model after replacing rare tokens is \",\n",
    "      cleaned_unigram_lm_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abe9eac",
   "metadata": {},
   "source": [
    "## 1.2 Bi-gram Language Model <a name='bigram_lm'></a>\n",
    "In the Bi-gram language model, we pick/generate next token conditioned only on the previous token. Therefore, for an arbitrary sequence $x_1x_2~...x_n$, its respective probability becomes:\n",
    "$$p(x_1x_2~...x_n) = p(x_1) ~\\Pi_{i=2} ^n p(x_i|x_{i-1})$$\n",
    "Let's use the same dataset (`Penn Treebank`) to evaluate this model's perplexity. (We use the dataset that already has the `<stop>` token at the end).\n",
    "\n",
    "We estimate $p(x_i|x_{i-1})$ as the $\\frac{count(x_{i-1},~x_i)}{count(x_{i-1})}$ according to the training dataset frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f96966fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T08:09:43.923182Z",
     "start_time": "2024-03-01T08:09:40.712043Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_first_order_conditional_probabilities(train_dataset: datasets.arrow_dataset.Dataset):\n",
    "    '''\n",
    "    In this function the conditional probabilities have to be computed based on train_dataset. The output of the\n",
    "    function is a dictionary having keys like (x_{i-1}, x_i) as a tuple and the value being p(x_i|x_{i-1}).\n",
    "    args:\n",
    "        train_dataset: a Dataset object that can be iterated to get all the sentences\n",
    "    output:\n",
    "        word_prob_dict: a dictionary containing the word probabilities\n",
    "        first_order_condition_prob: a dictionary containing the first order conditional probabilities\n",
    "                                    as discussed above.\n",
    "    '''\n",
    "    first_order_condition_prob = defaultdict(float) # in order to get zeroes \n",
    "    # let's first get the word frequencies (later used for computation of conditional probabilities)\n",
    "    word_prob_dict = get_word_probability_dict(train_dataset)\n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    for sample in train_dataset:\n",
    "        words = sample['sentence'].split()  # Tokenize the sentence into words\n",
    "        # Loop through each word in the sentence, starting from the second word\n",
    "        for i in range(1, len(words)):\n",
    "            # Get the current and previous words\n",
    "            prev_word = words[i - 1]\n",
    "            current_word = words[i]\n",
    "            # Increment the count of the current word given the previous word\n",
    "            first_order_condition_prob[(prev_word, current_word)] += 1\n",
    "    \n",
    "    # Compute probabilities\n",
    "    for (prev_word, current_word), count in first_order_condition_prob.items():\n",
    "        # from cond. probability formula above\n",
    "        first_order_condition_prob[(prev_word, current_word)] /= word_prob_dict[prev_word]\n",
    "\n",
    "    return word_prob_dict, first_order_condition_prob\n",
    "\n",
    "word_prob_dict, first_order_condition_prob = get_first_order_conditional_probabilities(ptb_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89cb406",
   "metadata": {},
   "source": [
    "#### Now let's analyze the Bi-gram language model for different sequences. We first create a function that can output the probability for a given string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e459cdc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T08:09:56.169914Z",
     "start_time": "2024-03-01T08:09:56.164907Z"
    }
   },
   "outputs": [],
   "source": [
    "def bigram_lm_seq_probability(input_sentence: str,\n",
    "                              word_prob_dict: dict,\n",
    "                              first_order_condition_prob: dict):\n",
    "    '''\n",
    "    args:\n",
    "        input_sentence: The input sequence string. Here we assume\n",
    "        word_prob_dict: a dictionary containing the word probabilities\n",
    "        first_order_condition_prob: a dictionary containing the first order conditional probabilities\n",
    "                                    as discussed in the previous function.\n",
    "    output:\n",
    "        probability: The probability of the input_sentence according to the Bi-gram language model\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    words = np.array(input_sentence.split())\n",
    "    probs = np.array([])\n",
    "    for i in range(1, len(words)):\n",
    "        np.append(probs, first_order_condition_prob.get((words[i-1], words[i]), 0))\n",
    "    \n",
    "    probability = word_prob_dict.get(words[0], 0) * np.prod(probs)\n",
    "\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87509ef",
   "metadata": {},
   "source": [
    "Let's investigate a major issue with higher order language models.\n",
    "#### Compute the probabilities for all the sequences in `ptb_test` dataset, and compute the minimum value among these probablities. What would be the perplexity for the dataset given these values?\n",
    "Discussion: How can we avoid this **overfitting** to train dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d9a0460",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T08:09:58.520917Z",
     "start_time": "2024-03-01T08:09:57.196662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% of samples in the test set have zero probability!\n"
     ]
    }
   ],
   "source": [
    "bigram_test_probabilities = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "for sample in ptb_test:\n",
    "    bigram_test_probabilities.append(bigram_lm_seq_probability(sample['sentence'], word_prob_dict, first_order_condition_prob))\n",
    "\n",
    "print(f\"{bigram_test_probabilities.count(0)/len(ptb_test)*100}% of samples in the test set have zero probability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ced914f",
   "metadata": {},
   "source": [
    "### Smoothing\n",
    "As we saw above, due to having new pair of consecutive words in the test dataset, we might have zero probabilities for some sequences. Therefore, as discussed in the lectures, in order to have a meaningful perplexity for N-gram language models, we need to smooth the probabilities to have non-zero values for non-seen sequences. In this exercise, we use Laplace smoothing as defined below:\n",
    "$$P(x_i|x_{i-1}) = \\frac{count(x_{i-1},~x_i) + \\alpha}{count(x_{i-1}) + \\alpha ~|V|}$$\n",
    ", where $\\alpha$ is the smoothing parameter, and $|V|$ is the (train dataset) vocabulary size.\n",
    "\n",
    "#### Let's recompute the conditional probabilities using Laplace smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d808944",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T08:14:05.994623Z",
     "start_time": "2024-03-01T08:14:05.987276Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_smoothed_first_order_conditional_probabilities(train_dataset: datasets.arrow_dataset.Dataset,\n",
    "                                                       smoothing_alpha: float):\n",
    "    '''\n",
    "    In this function the conditional probabilities have to be computed based on train_dataset. The output\n",
    "    of the function is a dictionary having keys like (x_{i-1}, x_i) as a tuple and the\n",
    "    value being p(x_i|x_{i-1}).\n",
    "    args:\n",
    "        train_dataset: a Dataset object that can be iterated to get all the sentences\n",
    "        smoothing_alpha: The alpha parameter used in the Laplace smoothing.\n",
    "    output:\n",
    "        word_prob_dict: a dictionary containing the word probabilities \n",
    "        first_order_condition_prob: a dictionary containing the smoothed first order\n",
    "                                    conditional probabilities as discussed above.\n",
    "    '''\n",
    "    first_order_condition_prob = defaultdict(float)  # Note that we shouldn't get zeros for unseen events.\n",
    "    # let's first get the word probabilities (later used for computation of conditional probabilities)\n",
    "    word_prob_dict = get_word_probability_dict(train_dataset)\n",
    "    \n",
    "    \n",
    "    # YOUR CODE here\n",
    "    for sample in train_dataset:\n",
    "        words = sample['sentence'].split()  # Tokenize the sentence into words\n",
    "        # Loop through each word in the sentence, starting from the second word\n",
    "        for i in range(1, len(words)):\n",
    "            # Get the current and previous words\n",
    "            prev_word = words[i - 1]\n",
    "            current_word = words[i]\n",
    "            # Increment the count of the current word given the previous word\n",
    "            first_order_condition_prob[(prev_word, current_word)] += 1\n",
    "    \n",
    "    # Compute probabilities\n",
    "    for (prev_word, current_word), count in first_order_condition_prob.items():\n",
    "        # from cond. probability formula above\n",
    "        first_order_condition_prob[(prev_word, current_word)] += smoothing_alpha\n",
    "        first_order_condition_prob[(prev_word, current_word)] /= (word_prob_dict[prev_word] + smoothing_alpha * len(word_prob_dict))\n",
    "\n",
    "    \n",
    "    return word_prob_dict, first_order_condition_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3008240c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T08:18:20.140805Z",
     "start_time": "2024-03-01T08:18:20.135111Z"
    }
   },
   "outputs": [],
   "source": [
    "def smoothed_bigram_lm_seq_probability(input_sentence: str,\n",
    "                                       word_prob_dict: dict,\n",
    "                                       word_frequency_dict: dict,\n",
    "                                       first_order_condition_prob: dict,\n",
    "                                       smoothing_alpha: float):\n",
    "    '''\n",
    "    args:\n",
    "        input_sentence: The input sequence string. Here we assume\n",
    "        word_prob_dict: a dictionary containing the word probabilities\n",
    "        word_frequency_dict: a dictionary containing the frequency for every word in vocabulary\n",
    "        first_order_condition_prob: a dictionary containing the first order conditional probabilities\n",
    "                                    as discussed in the previous function.\n",
    "        smoothing_alpha: The alpha parameter used in the Laplace smoothing.\n",
    "    output:\n",
    "        probability: The probability of the input_sentence according to the Bi-gram language model\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    words = np.array(input_sentence.split())\n",
    "    probs = np.array([])\n",
    "    for i in range(1, len(words)):\n",
    "        np.append(probs, first_order_condition_prob.get((words[i-1], words[i]), 0))\n",
    "    \n",
    "    probability = word_prob_dict.get(words[0], 0) * np.prod(probs)\n",
    "    \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf96a334",
   "metadata": {},
   "source": [
    "#### (self-created) Now let's formally evaluate the Bigram model in terms of perplexity. We first compute the entropy as the average negative log-likelihood:\n",
    "$$H(W_{\\text{test}}|M) = \\frac{1}{|W_{\\text{test}}|} \\sum_{i=1}^{|W_{\\text{test}}|} -\\log_2 P(w_i|w_{i-1}, M)$$\n",
    "\n",
    "where:\n",
    "- $W_{\\text{test}}$ is the input sequence.\n",
    "- $M$ is the Bigram language model.\n",
    "- $w_i$ represents the $i$-th word in the sequence.\n",
    "- $w_{i-1}$ represents the previous word to $w_i$.\n",
    "- $P(w_i|w_{i-1}, M)$ is the probability of word $w_i$ given the previous word $w_{i-1}$ according to the Bigram model $M$.\n",
    "\n",
    "In order to get a reliable value, we will do the above calculation for all the sentences in the `ptb_test` dataset, and then an average is taken over all these samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "199919cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T10:14:23.147725Z",
     "start_time": "2024-03-01T10:14:23.141312Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_bigram_lm_entropy(input_sentence: str,\n",
    "                           first_order_condition_prob: dict):\n",
    "    '''\n",
    "    args:\n",
    "        input_sentence: the input string that we would like to have its respective entropy value.\n",
    "        first_order_condition_prob: a dictionary containing the first order conditional probabilities\n",
    "                                    as discussed in the previous function.\n",
    "    output:\n",
    "        entropy: entropy value as defined above\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    words = np.array(input_sentence.split())    \n",
    "    logs = []\n",
    "    \n",
    "    for i in range(1, len(words)):\n",
    "        prev_word = words[i-1]\n",
    "        current_word = words[i]\n",
    "        logs.append(-np.log2(first_order_condition_prob.get((prev_word, current_word), 1)))\n",
    "        \n",
    "    entropy = np.mean(np.array(logs))\n",
    "    return entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "19c6406f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T10:03:37.753356Z",
     "start_time": "2024-03-01T10:03:37.747484Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_bigram_lm_perplexity(test_dataset: datasets.arrow_dataset.Dataset,\n",
    "                              first_order_condition_prob: dict):\n",
    "    '''\n",
    "    args:\n",
    "        test_dataset: the test dataset samples are used to compute the perplexity for the Unigram LM.\n",
    "        first_order_condition_prob: a dictionary containing the first order conditional probabilities\n",
    "                                    as discussed in the previous function.\n",
    "    output:\n",
    "        perplexity: entropy value as defined above\n",
    "    '''  \n",
    "    # YOUR CODE HERE       \n",
    "    test_dataset = np.array(test_dataset)\n",
    "    compute_entropy = np.vectorize(lambda sample: get_bigram_lm_entropy(sample['sentence'], first_order_condition_prob))\n",
    "    sentence_entropies = compute_entropy(test_dataset)\n",
    "    \n",
    "    perplexity = np.power(2, np.mean(sentence_entropies))\n",
    "\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd77e6c",
   "metadata": {},
   "source": [
    "#### Assuming $\\alpha=0.01$ for the smoothing, use the previous function and `bigram_lm_seq_probability` to compute the sequence probabilities for all the sentences in the `ptb_test` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e2c76c94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T08:54:42.032209Z",
     "start_time": "2024-03-01T08:54:36.270096Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% of samples in the test set have zero probability!\n"
     ]
    }
   ],
   "source": [
    "smoothed_bigram_test_probabilities = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "word_prob_dict, first_order_condition_prob = get_smoothed_first_order_conditional_probabilities(ptb_train, 0.01)\n",
    "\n",
    "word_frequency_dict = Counter()\n",
    "for sample in ptb_train:\n",
    "    word_frequency_dict.update(sample['sentence'].split())\n",
    "word_frequency_dict = dict(word_frequency_dict)\n",
    "\n",
    "for sample in ptb_test:\n",
    "    smoothed_bigram_test_probabilities.append(smoothed_bigram_lm_seq_probability(sample['sentence'], word_prob_dict, word_frequency_dict, first_order_condition_prob, 0.01))\n",
    "\n",
    "\n",
    "print(f\"{smoothed_bigram_test_probabilities.count(0)/len(ptb_test)*100}% of samples in the test set have zero probability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8b38b8",
   "metadata": {},
   "source": [
    "If the perplexity for a given sequence is computed as below, compute the Bigram language model perplexity over `ptb_test` dataset over all the sentences ($\\alpha=0.01)$:\n",
    "$$Perplexity(x_1x_2...x_n) = p(x_1x_2...x_n)^{-1/n}$$\n",
    ", where $p(x_1x_2...x_n)$ is the probability assigned to $x_1x_2...x_n$ sequence by the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7e3665f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T10:05:10.440380Z",
     "start_time": "2024-03-01T10:05:09.664293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram language model perplexity is 2.780068796985517\n"
     ]
    }
   ],
   "source": [
    "bigram_lm_perplexity = -1\n",
    "# YOUR CODE HERE\n",
    "bigram_lm_perplexity = get_bigram_lm_perplexity(ptb_test, first_order_condition_prob)\n",
    "\n",
    "print(f\"Bigram language model perplexity is {bigram_lm_perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd753ef",
   "metadata": {},
   "source": [
    "Repeat the same steps but for `cleaned_train_dataset` and `cleaned_test_dataset` datasets where rare tokens (with frequency less than 10) are replaced with `<unk>` token. Do we have a better or a worse perplexity compared to the previous computed perplexity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "92b98f8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T10:07:36.945934Z",
     "start_time": "2024-03-01T10:07:34.797912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(cleaned) Bigram language model perplexity is 0.8099554094584039\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "_, cleaned_first_order_condition_prob = get_smoothed_first_order_conditional_probabilities(cleaned_train_dataset, 0.01)\n",
    "cleaned_bigram_lm_perplexity = get_bigram_lm_perplexity(cleaned_test_dataset, cleaned_first_order_condition_prob)\n",
    "\n",
    "print(f\"(cleaned) Bigram language model perplexity is {cleaned_bigram_lm_perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ea844a",
   "metadata": {},
   "source": [
    "## 1.3 Tri-gram Language Model <a name='trigram_lm'></a>\n",
    "In the Tri-gram language model, we pick/generate next token conditioned only on the two previous tokens. Therefore, for an arbitrary sequence $x_1x_2~...x_n$, its respective probability becomes:\n",
    "$$p(x_1x_2~...x_n) = p(x_1) p(x_2|x_1) ~\\Pi_{i=3} ^n p(x_i|x_{i-2}x_{i-1})$$\n",
    "Let's use the same dataset (`Penn Treebank`) to evaluate this model's perplexity. (We use the dataset that already has the `<stop>` token at the end of each sentence).\n",
    "\n",
    "\n",
    "We estimate $p(x_i|x_{i-1}x_{i-2})$ using the Laplace smoothing with $\\alpha=3 \\cdot 10^{-3}$. First let's write a function that computes these conditional probabilities for the Tri-gram language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "abdf0b35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T09:01:23.701053Z",
     "start_time": "2024-03-01T09:01:23.694047Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_smoothed_second_order_conditional_probabilities(train_dataset: datasets.arrow_dataset.Dataset,\n",
    "                                                        smoothing_alpha: float):\n",
    "    '''\n",
    "    In this function the conditional probabilities have to be computed based on train_dataset. The output\n",
    "    of the function is a dictionary having keys like (x_{i-2}, x_{i-1}, x_i) as a tuple and the\n",
    "    value being p(x_i | x_{i-2} x_{i-1}).\n",
    "    args:\n",
    "        train_dataset: a Dataset object that can be iterated to get all the sentences\n",
    "        smoothing_alpha: The alpha parameter used in the Laplace smoothing.\n",
    "    output:\n",
    "        word_prob_dict: a dictionary containing the word probabilities \n",
    "        first_order_condition_prob: a dictionary containing the smoothed first order\n",
    "                                    conditional probabilities.\n",
    "        second_order_condition_prob: a dictionary containing the smoothed second order\n",
    "                                     conditional probabilities.\n",
    "    '''\n",
    "    second_order_condition_prob = defaultdict(float)  # Note that we shouldn't get zeros for unseen probabilies.\n",
    "    \n",
    "    # let's first get the 0th and 1st order conditional probabilities\n",
    "    (word_prob_dict, first_order_condition_prob) = get_smoothed_first_order_conditional_probabilities(\n",
    "        train_dataset, smoothing_alpha)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    for sample in train_dataset:\n",
    "        words = sample['sentence'].split()  # Tokenize the sentence into words\n",
    "        # Loop through each word in the sentence, starting from the second word\n",
    "        for i in range(2, len(words)):\n",
    "            prevprev_word = words[i - 2]\n",
    "            prev_word = words[i - 1]\n",
    "            current_word = words[i]\n",
    "            # Increment the count of the current word given the previous word\n",
    "            second_order_condition_prob[(prevprev_word, prev_word, current_word)] += 1\n",
    "    \n",
    "    # Compute probabilities\n",
    "    for (prevprev_word, prev_word, current_word), count in second_order_condition_prob.items():\n",
    "        # from cond. probability formula above\n",
    "        second_order_condition_prob[(prevprev_word, prev_word, current_word)] += smoothing_alpha\n",
    "        second_order_condition_prob[(prevprev_word, prev_word, current_word)] /= (first_order_condition_prob[prevprev_word, prev_word] + smoothing_alpha * len(word_prob_dict))\n",
    "    \n",
    "    return word_prob_dict, first_order_condition_prob, second_order_condition_prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936a664f",
   "metadata": {},
   "source": [
    "#### Now let's analyze the Tri-gram language model for different sequences. We first create a function that can output the probability for a given string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3f6a692c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T09:01:24.335972Z",
     "start_time": "2024-03-01T09:01:24.322144Z"
    }
   },
   "outputs": [],
   "source": [
    "def smoothed_trigram_lm_seq_probability(input_sentence: str,\n",
    "                                        word_prob_dict: dict,\n",
    "                                        word_frequency_dict: dict,\n",
    "                                        bigram_frequency_dict: dict,\n",
    "                                        first_order_condition_prob: dict,\n",
    "                                        second_order_condition_prob: dict,\n",
    "                                        smoothing_alpha: float):\n",
    "    '''\n",
    "    args:\n",
    "        input_sentence: The input sequence string. Here we assume\n",
    "        word_prob_dict: a dictionary containing the word probabilities\n",
    "        word_frequency_dict: a dictionary containing the frequency for every word in vocabulary\n",
    "        bigram_frequency_dict: a dictionary containing the frequency for every bigram in vocabulary\n",
    "        first_order_condition_prob: a dictionary containing the first order conditional probabilities\n",
    "                                    as discussed earlier.\n",
    "        second_order_condition_prob: a dictionary containing the second order conditional probabilities\n",
    "                                     as discussed in the previous function.\n",
    "    output:\n",
    "        probability: The probability of the input_sentence according to the Bi-gram language model\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    words = np.array(input_sentence.split())\n",
    "    probs = np.array([])\n",
    "    for i in range(2, len(words)):\n",
    "        np.append(probs, second_order_condition_prob.get((words[i-2], words[i-1], words[i]), 0))\n",
    "    \n",
    "    probability = word_prob_dict.get(words[0], 0) * first_order_condition_prob.get(words[1], 0) * np.prod(probs)\n",
    "\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9215fbd",
   "metadata": {},
   "source": [
    "#### (self-created) Now let's formally evaluate the Trigram model in terms of perplexity. We first compute the entropy as the average negative log-likelihood:\n",
    "$$H(W_{\\text{test}}|M) = \\frac{1}{|W_{\\text{test}}|} \\sum_{i=2}^{|W_{\\text{test}}|} -\\log_2 P(w_i|w_{i-1}, w_{i-2}, M)$$\n",
    "\n",
    "where:\n",
    "- $W_{\\text{test}}$ is the input sequence.\n",
    "- $M$ is the Trigram language model.\n",
    "- $w_i$ represents the $i$-th word in the sequence.\n",
    "- $w_{i-1}$ represents the previous word to $w_i$.\n",
    "- $w_{i-2}$ represents the previous word to $w_{i-1}$.\n",
    "- $P(w_i|w_{i-1}, w_{i-2}, M)$ is the probability of word $w_i$ given the previous words $w_{i-1}$ and $w_{i-2}$ according to the Trigram model $M$.\n",
    "\n",
    "In order to get a reliable value, we will do the above calculation for all the sentences in the `ptb_test` dataset, and then an average is taken over all these samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1a454bbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T10:15:18.046396Z",
     "start_time": "2024-03-01T10:15:18.040433Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_trigram_lm_entropy(input_sentence: str,\n",
    "                           second_order_condition_prob: dict):\n",
    "    '''\n",
    "    args:\n",
    "        input_sentence: the input string that we would like to have its respective entropy value.\n",
    "        second_order_condition_prob: a dictionary containing the second order conditional probabilities\n",
    "                                    as discussed in the previous function.\n",
    "    output:\n",
    "        entropy: entropy value as defined above\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    words = np.array(input_sentence.split())\n",
    "    \n",
    "    logs = []\n",
    "    for i in range(1, len(words)):\n",
    "        prevprev_word = words[i-2]\n",
    "        prev_word = words[i-1]\n",
    "        current_word = words[i]\n",
    "        logs.append(-np.log2(first_order_condition_prob.get((prevprev_word, prev_word, current_word), 1)))\n",
    "        \n",
    "    entropy = np.mean(np.array(logs))\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5d6b78e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T10:15:18.762402Z",
     "start_time": "2024-03-01T10:15:18.756924Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_trigram_lm_perplexity(test_dataset: datasets.arrow_dataset.Dataset,\n",
    "                              second_order_condition_prob: dict):\n",
    "    '''\n",
    "    args:\n",
    "        test_dataset: the test dataset samples are used to compute the perplexity for the Unigram LM.\n",
    "        first_order_condition_prob: a dictionary containing the second order conditional probabilities\n",
    "                                    as discussed in the previous function.\n",
    "    output:\n",
    "        perplexity: entropy value as defined above\n",
    "    '''  \n",
    "    # YOUR CODE HERE       \n",
    "    test_dataset = np.array(test_dataset)\n",
    "    compute_entropy = np.vectorize(lambda sample: get_trigram_lm_entropy(sample['sentence'], second_order_condition_prob))\n",
    "    sentence_entropies = compute_entropy(test_dataset)\n",
    "    \n",
    "    perplexity = np.power(2, np.mean(sentence_entropies))\n",
    "\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16865742",
   "metadata": {},
   "source": [
    "#### Now let's compute the probability for sequences in the test dataset, assuming $\\alpha=3\\cdot10^{-3}$ has been used in the Laplace smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "36feefd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T09:02:51.737544Z",
     "start_time": "2024-03-01T09:02:41.933685Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0% of samples in the test set have zero probability!\n"
     ]
    }
   ],
   "source": [
    "smoothed_trigram_test_probabilities = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "word_prob_dict, first_order_condition_prob, second_order_condition_prob = get_smoothed_second_order_conditional_probabilities(ptb_train, 0.003)\n",
    "\n",
    "word_frequency_dict = Counter()\n",
    "for sample in ptb_train:\n",
    "    word_frequency_dict.update(sample['sentence'].split())\n",
    "word_frequency_dict = dict(word_frequency_dict)\n",
    "\n",
    "bigram_frequency_dict = defaultdict(int)\n",
    "for sample in ptb_train:\n",
    "        words = sample['sentence'].split()  # Tokenize the sentence into words\n",
    "        # Loop through each word in the sentence, starting from the second word\n",
    "        for i in range(1, len(words)):\n",
    "            # Get the current and previous words\n",
    "            prev_word = words[i - 1]\n",
    "            current_word = words[i]\n",
    "            # Increment the count of the current word given the previous word\n",
    "            bigram_frequency_dict[(prev_word, current_word)] += 1\n",
    "\n",
    "for sample in ptb_test:\n",
    "    smoothed_trigram_test_probabilities.append(smoothed_trigram_lm_seq_probability(sample['sentence'], word_prob_dict, word_frequency_dict, bigram_frequency_dict, first_order_condition_prob, second_order_condition_prob, 0.003))\n",
    "\n",
    "\n",
    "print(f\"{smoothed_trigram_test_probabilities.count(0)/len(ptb_test)*100}% of samples in the test set have zero probability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ddd040",
   "metadata": {},
   "source": [
    "Now we compute the perplexity on the `ptb_test` dataset for the tri-gram language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "25ed3d70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T10:16:37.610591Z",
     "start_time": "2024-03-01T10:16:36.786654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram language model perplexity is 1.0\n"
     ]
    }
   ],
   "source": [
    "trigram_lm_perplexity = -1\n",
    "# YOUR CODE HERE\n",
    "trigram_lm_perplexity = get_trigram_lm_perplexity(ptb_test, second_order_condition_prob)\n",
    "\n",
    "print(f\"Trigram language model perplexity is {trigram_lm_perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34c59c8",
   "metadata": {},
   "source": [
    "Repeat the same steps but for `cleaned_train_dataset` and `cleaned_test_dataset` datasets where rare tokens (with frequency less than 10) are replaced with `<unk>` token. Do we have a better or a worse perplexity compared to the previous computed perplexity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "41b456be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T10:19:12.087596Z",
     "start_time": "2024-03-01T10:19:08.643809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(cleaned) Trigram language model perplexity is 1.0\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "_, _, cleaned_second_order_condition_prob = get_smoothed_second_order_conditional_probabilities(cleaned_train_dataset, 0.01)\n",
    "cleaned_trigram_lm_perplexity = get_trigram_lm_perplexity(cleaned_test_dataset, cleaned_second_order_condition_prob)\n",
    "\n",
    "print(f\"(cleaned) Trigram language model perplexity is {cleaned_trigram_lm_perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8e69cc",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    " - How are the three discussed models performance compare to each other?\n",
    " - What is the cost of using N-gram language models for even larger N values?\n",
    " - What is the effect of vocabulary size on models' perplexities? Can we compare models with different vocabulary sizes?\n",
    " - What is the perplexity of a language model (vocabulary size of |V|) that given any context (i.e., $x_1 x_2 ... x_{n-1}$) assigns uniform probabilities (for all the tokens in the vocabulary) for the next token? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641578b1",
   "metadata": {},
   "source": [
    "## 2. Task B: Neural Language Models <a name='neural_lm'></a>\n",
    "\n",
    "In this exercise, we will better understand the functioning of some simple neural language models. We first start with a fixed-window neural language model. In the following subsection, we will investigate an RNN-based language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cee765",
   "metadata": {},
   "source": [
    "### 2.1 Fixed-Window Neural Language Model <a name='fixed_window_lm'></a>\n",
    "This language model take as input a constant number of tokens, and then outputs a probability distribution for the next token. In this section, we assume the underlying model is a Multi-layer Perceptron (MLP) with a single hidden layer. This model doesn't have the sparsity issue of N-gram language models, but is always limited to a fixed window of tokens.\n",
    "\n",
    "In this section, we don't include the training of the model but rather we use a pretrained model on the same training dataset. We evaluate the language model over the `ptb_test` dataset, to show the power of neural language models, when compared to N-gram language models.\n",
    "\n",
    "More importantly, we use PyTorch modules in this section, so that you get more familiar with its capabilities. Throughout this exercise, we use a `window_size=3` for this model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6808da1e",
   "metadata": {},
   "source": [
    "Let's first create a dataset of all `window_size` consecutive tokens from the `ptb_train` dataset. you can read more about PyTorch datasets and how to create a custom dataset  [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "70f5c3bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T09:36:16.084865Z",
     "start_time": "2024-03-01T09:36:16.071780Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "window_size = 3\n",
    "vocabulary_size = 10000\n",
    "word_emb_dim = 100\n",
    "hidden_dim = 100\n",
    "\n",
    "\n",
    "class FixedWindowDataset(Dataset):\n",
    "    # read more about custom datasets at https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "    def __init__(self,\n",
    "                 train_dataset: datasets.arrow_dataset.Dataset,\n",
    "                 test_dataset: datasets.arrow_dataset.Dataset,\n",
    "                 window_size: int,\n",
    "                 vocabulary_size: int\n",
    "                ):\n",
    "        self.prepared_train_dataset = self.prepare_fixed_window_lm_dataset(train_dataset, window_size + 1)\n",
    "        self.prepared_test_dataset = self.prepare_fixed_window_lm_dataset(test_dataset, window_size + 1)\n",
    "        \n",
    "        dataset_vocab = self.get_dataset_vocabulary(train_dataset)\n",
    "        # defining a dictionary that simply maps tokens to their respective index in the embedding matrix\n",
    "        self.word_to_index = {word: idx for idx,word in enumerate(dataset_vocab)}\n",
    "        self.index_to_word = {idx: word for idx,word in enumerate(dataset_vocab)}\n",
    "        \n",
    "        assert vocabulary_size >= len(dataset_vocab) , f\"The dataset vocab size is {len(dataset_vocab)}!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prepared_train_dataset)\n",
    "    \n",
    "    def get_encoded_test_samples(self):\n",
    "        all_token_lists = [sample.split() for sample in self.prepared_test_dataset]\n",
    "        all_token_ids = [[self.word_to_index.get(word, self.word_to_index[\"<unk>\"])\n",
    "                          for word in token_list[:-1]]\n",
    "                         for token_list in all_token_lists\n",
    "                        ]\n",
    "        all_next_token_ids = [self.word_to_index.get(token_list[-1], self.word_to_index[\"<unk>\"]) for \n",
    "                              token_list in all_token_lists] #list of index of last token of each sentence/token_list\n",
    "        return torch.tensor(all_token_ids), torch.tensor(all_next_token_ids)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # here we need to transform the data to the format we expect at the model input\n",
    "        token_list = self.prepared_train_dataset[idx].split()\n",
    "        # having a fallback to <unk> token if an unseen word is encoded.\n",
    "        token_ids = [self.word_to_index.get(word, self.word_to_index[\"<unk>\"]) for word in token_list[:-1]]\n",
    "        next_token_id = self.word_to_index.get(token_list[-1], self.word_to_index[\"<unk>\"])\n",
    "        return torch.tensor(token_ids), torch.tensor(next_token_id)\n",
    "    \n",
    "    def decode_idx_to_word(self, token_id):\n",
    "        return [self.index_to_word[id_.item()] for id_ in token_id]\n",
    "    \n",
    "    def get_dataset_vocabulary(self, train_dataset: datasets.arrow_dataset.Dataset):\n",
    "        vocab = sorted(set(\" \".join([sample[\"sentence\"] for sample in train_dataset]).split()))\n",
    "        # we also add a <start> token to include initial tokens in the sentences in the dataset\n",
    "        vocab += [\"<start>\"]\n",
    "        return vocab\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_fixed_window_lm_dataset(target_dataset: datasets.arrow_dataset.Dataset,\n",
    "                                        window_size: int):\n",
    "        '''\n",
    "        Please note that for the very first tokens, they will be added like \"<start> <start> Token#1\".\n",
    "        args:\n",
    "            target_dataset: the target dataset where its consecutive tokens of length 'window_size' should be extracted\n",
    "            window_size: the window size for the language model\n",
    "        output:\n",
    "            prepared_dataset: a list of strings each containing 'window_size' tokens.\n",
    "        '''\n",
    "        \n",
    "        prepared_dataset = []\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        for sample in target_dataset:\n",
    "            words = sample['sentence'].split()  # Split the sequence into individual words\n",
    "            words.insert(0, \"<start>\")\n",
    "            words.insert(1, \"<start>\")\n",
    "            \n",
    "            for i in range(len(words) - window_size + 1):\n",
    "                window = words[i:i+window_size]\n",
    "                prepared_dataset.append(' '.join(window))\n",
    "        \n",
    "        prepared_dataset = [token for sublist in prepared_dataset for token in sublist]\n",
    "        \n",
    "        return prepared_dataset\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "524f4a18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T09:36:20.350867Z",
     "start_time": "2024-03-01T09:36:16.835954Z"
    }
   },
   "outputs": [],
   "source": [
    "fixed_window_dataset = FixedWindowDataset(ptb_train, ptb_test, window_size, vocabulary_size)\n",
    "\n",
    "# let's create a simple dataloader for this dataset\n",
    "train_dataloader =  DataLoader(fixed_window_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b44e90d",
   "metadata": {},
   "source": [
    "Now, let's define the underlying PyTorch model for the language model. You can read more about PyTorch models [here](https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html).\n",
    "\n",
    "**Note**: Here in the forward pass, we compute the negative log-likelihood after passing through the MLP layers. Here we use `torch.nn.LogSoftmax`, as it's numerically more stable than doing seperately `softmax` followed by taking its logarithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e3ea691f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T09:36:23.424675Z",
     "start_time": "2024-03-01T09:36:23.417437Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class Fixed_window_language_model(torch.nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_dim, window_size, vocab_size=10000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.word_embeddings = torch.nn.Embedding(vocab_size, emb_dim) # word embeddings\n",
    "        self.linear1 = torch.nn.Linear(window_size * emb_dim, hidden_dim) # first linear layer\n",
    "        self.activation_func = torch.tanh # the activation function\n",
    "        self.linear2 = torch.nn.Linear(hidden_dim, vocab_size) # second linear layer\n",
    "        \n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n",
    "        self.criterion = torch.nn.NLLLoss()\n",
    "     \n",
    "    def forward(self, input_ids, labels):\n",
    "        inputs_embeds = self.word_embeddings(input_ids)\n",
    "        concat_input_embed = inputs_embeds.reshape(-1, self.emb_dim * self.window_size)\n",
    "        hidden_state = self.activation_func( self.linear1(concat_input_embed) )\n",
    "        logits = self.log_softmax( self.linear2(hidden_state) )\n",
    "        loss = self.criterion(logits, labels)\n",
    "        \n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dae5397",
   "metadata": {},
   "source": [
    "Now let's see how easy it is to train a model with PyTorch! (we provide a trained model in the cell after train, so that you can just start using the model without going through the time-consuming training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "93779901",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T09:36:24.390614Z",
     "start_time": "2024-03-01T09:36:24.343538Z"
    }
   },
   "outputs": [],
   "source": [
    "# defining the model\n",
    "model_fixed_window = Fixed_window_language_model(emb_dim=word_emb_dim, hidden_dim=hidden_dim,\n",
    "                                                 window_size=window_size, vocab_size=vocabulary_size)\n",
    "\n",
    "# defining the optimizer\n",
    "optimizer = optim.SGD(model_fixed_window.parameters(),\n",
    "                      lr=0.005,\n",
    "                      momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "77626d29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T09:36:26.194126Z",
     "start_time": "2024-03-01T09:36:25.029433Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):  \u001b[38;5;66;03m# loop over the dataset multiple times\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;66;03m# get the inputs; data is a tuple of (context, target)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m         context, target \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;66;03m# zero the parameter gradients\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[73], line 45\u001b[0m, in \u001b[0;36mFixedWindowDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# having a fallback to <unk> token if an unseen word is encoded.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_to_index\u001b[38;5;241m.\u001b[39mget(word, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_to_index[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<unk>\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m token_list[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m---> 45\u001b[0m next_token_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_to_index\u001b[38;5;241m.\u001b[39mget(token_list[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_to_index[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<unk>\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(token_ids), torch\u001b[38;5;241m.\u001b[39mtensor(next_token_id)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        # get the inputs; data is a tuple of (context, target)\n",
    "        context, target = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        loss = model_fixed_window(context, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 5000 == 4999. :    # print every 5000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 5000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# saving the trained model\n",
    "torch.save(model_fixed_window.state_dict(), \"fixed_window_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0483dd01",
   "metadata": {},
   "source": [
    "We provide a trained model, so that you can start using it right away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "54ba51b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T09:29:57.053254Z",
     "start_time": "2024-03-01T09:29:56.861257Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'fixed_window_model.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m fixed_window_checkpoint_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfixed_window_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m model_fixed_window\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(fixed_window_checkpoint_file))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fixed_window_model.pt'"
     ]
    }
   ],
   "source": [
    "fixed_window_checkpoint_file = \"fixed_window_model.pt\"\n",
    "model_fixed_window.load_state_dict(torch.load(fixed_window_checkpoint_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b806f375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context and 'target' ids (target is the next word after the context)\n",
    "test_token_ids, test_target_ids = fixed_window_dataset.get_encoded_test_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87e283d",
   "metadata": {},
   "source": [
    "We now have the `test_token_ids`, `test_target_ids` tensors for the test dataset. The `test_token_ids` are the context ids and `test_target_ids` are the respective **next token** (a.k.a. target here) for these contexts.\n",
    "#### Using the trained model, implement a function that can output the loss for the discussed test dataset. How can we generally decide if the model is overfitted to the train dataset or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e352e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_dataset_loss(model: torch.nn.Module,\n",
    "                               test_token_ids: torch.Tensor,\n",
    "                               test_target_ids: torch.Tensor):\n",
    "    '''\n",
    "    args:\n",
    "        model: fixed-window language model\n",
    "        test_token_ids: the context ids in a single tensor.\n",
    "        test_target_ids: the target ids (next token after the context) in a single tensor.\n",
    "    output:\n",
    "        avg_test_loss: The average loss of model over test dataset.\n",
    "    '''\n",
    "    batch_size = 4\n",
    "    test_loss = []\n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return avg_test_loss\n",
    "\n",
    "\n",
    "test_dataset_loss = generate_test_dataset_loss(model_fixed_window, test_token_ids, test_target_ids)\n",
    "print(f\"Test dataset loss is {test_dataset_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085d7476",
   "metadata": {},
   "source": [
    "#### Using the trained fixed-window model, implemention a function that can output entropy for a given sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db54e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seqeuence_entropy_fixed_window_lm(model: torch.nn.Module,\n",
    "                                              input_sequence: str,\n",
    "                                              window_size: int,\n",
    "                                              word_to_idx: dict):\n",
    "    '''\n",
    "    Note that e.g., in order to get the first token probability, you need to pass a sequence\n",
    "    like \"<start> <start> <start>\" (prefix padding) to the neural model. In a similar fashion, we need to pass\n",
    "    \"<start> <start> TOKEN#1\" for getting the probability of the second token.\n",
    "    args:\n",
    "        model: fixed-window language model\n",
    "        input_sequence: the sequence for which we want to calculate the probability\n",
    "        window_size: the size of window for the language model\n",
    "        word_to_idx: a mapping from words to the embedding indices (to encode tokens before being\n",
    "                     passed to model). You can get this dict from 'fixed_window_dataset.word_to_index'\n",
    "    output:\n",
    "        sequence_entropy: the entropy for the input sequence using the trained model\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return sequence_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d055835",
   "metadata": {},
   "source": [
    "#### Compute the perplexity for the trained fixed-window language model over `ptb_test` dataset using the previous function. How does it perform compared to N-gram language models we discussed earlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4289886",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = -1\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(f\"The fixed-window model perplexity over test dataset is {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240c3d89",
   "metadata": {},
   "source": [
    "### 2.2 RNN-based Language Model <a name='rnn_lm'></a>\n",
    "To address the need for a neural architecture that can proceed with any length input (as opposed to the fixed-window model that can only process a fixed number of tokens), we implement the Recurrent Neural Network (RNN). The core idea behind is that we can apply the same weight W repeatedly.\n",
    "\n",
    "An advatange of RNN model compared to fixed-window langauage model is that we can pass a given sentence at once, instead of passing it in many windows of size `window_size`. Moreover, the language model has the ability to look behind further that a fixed number of tokens.\n",
    "\n",
    " As we already did a neural model training exercise for the previous neural model, we only provide a trained LM at this section, so that you can focus only on the analysis part.\n",
    " \n",
    "You can find the dataset structure as well as the RNN architecture in the `rnn_utils.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc631bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn_utils import RNNDataset, RNN_language_model\n",
    "\n",
    "vocabulary_size = 10000\n",
    "word_emb_dim = 200\n",
    "hidden_dim = 200\n",
    "\n",
    "rnn_dataset = RNNDataset(ptb_train, ptb_test, vocabulary_size)\n",
    "\n",
    "# if gpu is available, we puts the model on it \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Here we need a <pad> token for the RNN model, in order to have a batch of sequences with difference sizes \n",
    "pad_idx = rnn_dataset.pad_idx # the index for <pad> token\n",
    "rnn_model = RNN_language_model(vocab_size=vocabulary_size, emb_dim=word_emb_dim, hidden_dim=hidden_dim,\n",
    "                               pad_idx=pad_idx)\n",
    "rnn_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25137a9f",
   "metadata": {},
   "source": [
    "load the model weights using the state_dict in `rnn_model.pt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52adb1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06eb967",
   "metadata": {},
   "source": [
    "As the training of an RNN model is time-consuming, we provide a trained language model on this dataset (`rnn_model.pt`), so that you can just analyze the model performance here.\n",
    "As mentioned above, as RNN can get sequences with varying lengths, the input sequences should be padded with a special token like `<pad>`, so that we can create a batch of sentences. The output of the defined RNN model (see the architecture detail `rnn_utils.py`) is the model's entropy over the input data.\n",
    "\n",
    "#### First get the encoded test samples of `ptb_test` dataset, and then pass these (already padded) sentences to the RNN model to get the respective entropy values. Compute the perplexity of the model and compare it with previous approaches.\n",
    "**HINT**: You can use the `get_encoded_test_samples` function of `rnn_dataset` to get encoded test samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d7b99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perplexity = -1\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(f\"The model perplexity is {test_perplexity}\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a8ced340a52f9326f5856e1d63a73f97bd9f0a225610b549ff7b502d766a19ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
